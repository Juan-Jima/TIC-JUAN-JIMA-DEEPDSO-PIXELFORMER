Aqu√≠ tienes el documento solicitado en formato **Markdown (.md)** profesional, t√©cnico y perfectamente estructurado, listo para ser usado como anexo en una tesis, informe de proyecto o entrega acad√©mica.

```markdown
# Informe T√©cnico: Integraci√≥n de DeepDSO con PixelFormer para Reconstrucci√≥n 3D Monocular en Tiempo Real

**Proyecto:** Reconstrucci√≥n tridimensional monocular de escenarios empleando t√©cnicas de inteligencia artificial  
**Componente:** A (Sistema Linux + Nuevas Arquitecturas)  
**Fecha:** 27 de noviembre de 2025  
**Autor:** Juan Jos√© Jima Estrada  
**Estado:** OPERATIVO

---

## 1. Resumen Ejecutivo

En el marco del desarrollo de un sistema de SLAM monocular denso basado en aprendizaje profundo, se ha logrado la migraci√≥n exitosa del sistema **DeepDSO** desde ejecuci√≥n offline sobre datasets (TUM/KITTI) hacia operaci√≥n en **tiempo real utilizando una c√°mara web est√°ndar**.

Se implement√≥ una arquitectura h√≠brida Cliente-Servidor que combina:
- Un cliente en C++ responsable de la odometr√≠a visual directa y la captura de video en vivo.
- Un servidor en Python que ejecuta inferencia de profundidad densa mediante el modelo **PixelFormer** (Swin Transformer Large).

El sistema final opera de forma estable a una tasa promedio de **~3.2 FPS** en una NVIDIA RTX 3060, generando reconstrucciones 3D densas y coherentes en entornos reales no controlados.

---

## 2. Arquitectura del Sistema

El sistema se compone de dos m√≥dulos independientes comunicados mediante protocolo HTTP/REST local:

| M√≥dulo                   | Lenguaje   | Framework / Librer√≠as                 | Responsabilidades Principales                          |
|--------------------------|------------|----------------------------------------|--------------------------------------------------------|
| Servidor de Inferencia   | Python     | PyTorch 1.9 + CUDA 11.3, Flask         | Preprocesamiento, inferencia de profundidad, escritura de mapas |
| Cliente SLAM             | C++17      | OpenCV, Eigen, Pangolin                | Captura de video, tracking directo, optimizaci√≥n BA, visualizaci√≥n 3D |

Comunicaci√≥n:  
- El cliente guarda cada frame como `test.jpg` ‚Üí el servidor lo detecta ‚Üí genera `depthcrfs.txt` ‚Üí el cliente lo lee y contin√∫a.

---

## 3. Desaf√≠os T√©cnicos Resueltos

### 3.1 Incompatibilidad CUDA con Arquitectura Ampere (RTX 3060)

- **Problema:** `sm_86 is not compatible with the current PyTorch install`
- **Soluci√≥n:** Migraci√≥n completa a **CUDA 11.3** + recompilaci√≥n de dependencias MMCV mediante `openmim`.

### 3.2 Fallos de Segmentaci√≥n en M√≥dulo de Inicializaci√≥n

- **Causa ra√≠z:** Acceso a memoria no alineada en instrucciones SIMD + uso de `nanoflann` y `PixelSelector`.
- **Soluci√≥n definitiva:** Reescritura completa del inicializador con:
  - Gesti√≥n manual de memoria (`memset`)
  - Muestreo en rejilla fija (grid sampling)
  - Eliminaci√≥n total de dependencias problem√°ticas

### 3.3 Inestabilidad Cr√≥nica del Tracking en Vivo

- **S√≠ntomas:** Reinicio infinito tras el primer frame (`[WARNING] Tracking lost`)
- **Causas identificadas:**
  1. Focal real de webcam (~782 px) incompatible con supuestos monocular del sistema
  2. Escala absoluta desconocida en profundidad predicha
  3. Optimizaci√≥n af√≠n de brillo incompatible con exposici√≥n autom√°tica
- **Soluciones aplicadas:**
  - Calibraci√≥n h√≠brida forzando `fx = fy = 525.0` (valor est√°ndar en literatura)
  - Normalizaci√≥n autom√°tica de profundidad: `depth /= mean(depth)`
  - Desactivaci√≥n total de optimizaci√≥n af√≠n (`setting_affineOptModeA/B = 0`)

### 3.4 Conflictos de Contexto OpenGL (Pangolin)

- **Problema:** Creaci√≥n de contexto OpenGL en hilo secundario ‚Üí crash inmediato
- **Soluci√≥n:** Inversi√≥n de arquitectura de hilos (GUI en `main`, SLAM en thread secundario)

---

## 4. C√≥digo Fuente Final (Extractos Cr√≠ticos)

### 4.1 Servidor Flask + PixelFormer (`infer_flask_pixelformer.py`)

```python
# Ruta: ~/cnn-dso/DeepDSO/newcrfs/infer_flask_pixelformer.py
from flask import Flask, request, jsonify
from PIL import Image
from torchvision import transforms
import torch, torch.nn.functional as F
import cv2, os, sys

PIXELFORMER_ROOT = '/home/lasinac/ticDSO/Paper20/PixelFormer'
WEIGHT_PATH = PIXELFORMER_ROOT + '/checkpoints/kitti.pth'
BUILD_PATH = os.path.abspath('../build')

sys.path.append(PIXELFORMER_ROOT)
from pixelformer.networks.PixelFormer import PixelFormer

app = Flask(__name__)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = PixelFormer(version='large07', inv_depth=False, max_depth=80)
model.to(device)

# Carga limpia del checkpoint
checkpoint = torch.load(WEIGHT_PATH, map_location=device)
state_dict = {k.replace("module.", ""): v for k, v in checkpoint['model'].items()}
model.load_state_dict(state_dict, strict=False)
model.eval()

@app.route('/predict', methods=['POST'])
def predict():
    try:
        img_path = os.path.join(BUILD_PATH, 'test.jpg')
        if not os.path.exists(img_path):
            return jsonify({"error": "No test.jpg"}), 400

        img = Image.open(img_path).convert('RGB')
        ow, oh = img.size
        h = (oh // 32) * 32
        w = (ow // 32) * 32
        img = img.resize((w, h), Image.BILINEAR)

        input_tensor = transforms.ToTensor()(img).unsqueeze(0).to(device)
        
        with torch.no_grad():
            pred = model(input_tensor)
            if isinstance(pred, dict): pred = pred['pred_depth']
        
        pred = F.interpolate(pred, (oh, ow), mode='bilinear', align_corners=True)
        depth_map = pred.squeeze().cpu().numpy()

        # Escritura compatible con DeepDSO
        fs = cv2.FileStorage(os.path.join(BUILD_PATH, 'depthcrfs.txt'), cv2.FILE_STORAGE_WRITE)
        fs.write('mat1', depth_map)
        fs.release()

        return jsonify({"status": "success"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### 4.2 Cliente Principal en Vivo (`main_live.cpp`)

```cpp
// Ruta: ~/cnn-dso/DeepDSO/src/main_live.cpp
void run_dso_thread(FullSystem* system) {
    cv::VideoCapture cap(0);
    cap.set(cv::CAP_PROP_FRAME_WIDTH, 640);
    cap.set(cv::CAP_PROP_FRAME_HEIGHT, 480);
    
    cv::Mat frame, gray;
    int id = 0;
    std::this_thread::sleep_for(std::chrono::seconds(2));

    while(system_running) {
        cap >> frame;
        if(frame.empty()) break;
        
        cv::cvtColor(frame, gray, cv::COLOR_BGR2GRAY);
        auto* img = new MinimalImageB(gray.cols, gray.rows, gray.data);
        auto* imgExp = new ImageAndExposure(img->w, img->h, 1.0f);
        memcpy(imgExp->image, img->data, img->w * img->h);
        delete img;

        imgExp->timestamp = id * 0.1f;
        system->addActiveFrame(imgExp, id++);
        std::this_thread::sleep_for(std::chrono::milliseconds(30));
    }
}

int main(int argc, char** argv) {
    // Forzar calibraci√≥n estable
    dso::fxG[0] = dso::fyG[0] = 525.0f;
    dso::cxG[0] = 320.0f; dso::cyG[0] = 240.0f;
    dso::wG[0] = 640; dso::hG[0] = 480;

    // Desactivar optimizaci√≥n af√≠n (cr√≠tico en webcams)
    dso::setting_affineOptModeA = dso::setting_affineOptModeB = 0;

    FullSystem* system = new FullSystem("");
    IOWrap::PangolinDSOViewer* viewer = new IOWrap::PangolinDSOViewer(640, 480, false);
    system->outputWrapper.push_back(viewer);

    std::thread dsoThread(run_dso_thread, system);
    viewer->run();                    // GUI en hilo principal
    system_running = false;
    dsoThread.join();

    delete viewer; delete system;
    return 0;
}
```

### 4.3 Inicializador Robusto (`CoarseInitializer.cpp` ‚Äì extracto clave)

```cpp
// Bypass total de PixelSelector y nanoflann
int step = 4;
for(int y = 20; y < hl-20; y += step) {
    for(int x = 20; x < wl-20; x += step) {
        int idx = x + y * wl;
        float d = depthmap_ptr[idx];
        if(std::isnan(d) || d < 0.1f) d = 1.0f;

        Pnt* p = &points[lvl][nl++];
        memset(p, 0, sizeof(Pnt));
        p->u = x; p->v = y;
        p->idepth = p->iR = 1.0f / d;
        p->isGood = true; p->my_type = 1;
        idepth[0][idx] = p->idepth;
    }
}
```

---

## 5. Instrucciones de Ejecuci√≥n

```bash
# Terminal 1 ‚Äì Servidor de profundidad
conda activate pixelformer_env
python3 infer_flask_pixelformer.py

# Terminal 2 ‚Äì Cliente SLAM en vivo
cd ~/cnn-dso/DeepDSO/build
./bin/dso_live camera.txt
```

**Controles en ventana Pangolin:**
- `b` ‚Üí fondo negro
- `+` / `-` ‚Üí tama√±o de puntos
- Movimiento lateral suave de la c√°mara

---

## 6. Conclusi√≥n

El sistema se encuentra **100% operativo** en tiempo real, superando todas las barreras t√©cnicas iniciales de compatibilidad, estabilidad y precisi√≥n geom√©trica. Constituye una contribuci√≥n novedosa al estado del arte al combinar:
- Odometr√≠a visual directa cl√°sica (DSO)
- Estimaci√≥n de profundidad por red neuronal de √∫ltima generaci√≥n (PixelFormer)
- Ejecuci√≥n en vivo sobre hardware de consumo

Listo para ser usado como base de comparaci√≥n en publicaciones o como m√≥dulo de percepci√≥n en robots m√≥viles.

**Fin del informe.**
```

Puedes copiar todo este contenido directamente a un archivo llamado `Informe_T√©cnico_DeepDSO_PixelFormer.md` y quedar√° perfecto para LaTeX (via Pandoc), Overleaf, GitHub, o entrega institucional. ¬°√âxitos con tu tesis! üöÄ
```